{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export BHSA Data for Processing\n",
    "\n",
    "Use Text-Fabric to export a time-expression (timex) dataset and a verb dataset.\n",
    "\n",
    "In all cases, the data is only selected from Biblical Hebrew.\n",
    "\n",
    "## 1. Dataset for Time Expressions\n",
    "* Select phrases with the function of Time.\n",
    "    * Exclude phrases that occur in clauses with multiple time-function phrases.\n",
    "* Features of the phrase\n",
    "    * lexeme/count of prepositions, substantives, and quantifiers (cardinal numbers).\n",
    "    * part of speech patterns within the phrase\n",
    "* Features of the enclosing clause\n",
    "    * domain\n",
    "    * lex and tense of verb if present\n",
    "    * position of phrase in relation to the verb \n",
    "    * position of phrase in relation to the clause\n",
    "    * main vs. subordinate\n",
    "    \n",
    "### 1.1. Classification of Time Expressions\n",
    "* See [time_phrase_analysis.ipynb]('../data_analysis/time_phrase_analysis.ipynb') for the rationale behind the classifications performed in this section.\n",
    "* a spreadsheet with prepositions and adverbs/substantives is exported for manual annotation of their semantic content; the annotations for prepositions + adverbs/substantives are used to establish a unique classification of the time phrase.\n",
    "* Text-Fabric Search is used to identify phrases that contain the desired preposition adverb/substantive combinations. TF Search is ideal since it can easily represent the phrase internal structure of the patterns while also ignoring adnominal elements that intervene. The search templates also allow for the method of classification to be easily scrutinized and adjusted. \n",
    "* Finally, the template-processed phrases are combined with the manual annotations. A single tag is produced and added to the timex dataset for qualifying phrases: `timex.group`. \n",
    "    \n",
    "    \n",
    "## 2. Dataset for Verbs\n",
    "\n",
    "In order to compare the time phrase dataset with the broader tendencies of verbs in the Hebrew Bible, another export contains:\n",
    "* **Features of verbs in the HB**\n",
    "    * verb must be in a predicate phrase\n",
    "    * tense and lexeme\n",
    "    <br><br>\n",
    "* **Features of the enclosing clause**\n",
    "    * domain\n",
    "    * position of verb's phrase in relation to the clause\n",
    "    * main vs. subordinate\n",
    "    \n",
    "## 3. Dataset for All Clauses\n",
    "* **A simple export where each row represents a clause, has a T/F value for the presence of a time phrase**.\n",
    "    * Include book, chapter, and verse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import csv, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put custom functions in path\n",
    "if __name__ == '__main__' and __package__ is None:\n",
    "    os.sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "114 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='~/github/etcbc/bhsa/tf', modules='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.01s B book                 from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B chapter              from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B verse                from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.09s B function             from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.05s B kind                 from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.18s B pdp                  from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.26s B sp                   from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.30s B typ                  from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.24s B rela                 from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.15s B ls                   from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B gloss                from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.03s B domain               from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.26s B vt                   from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.17s B lex                  from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.30s B number               from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.14s B language             from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 108 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  9.41s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "                 book chapter verse\n",
    "                 function kind\n",
    "                 pdp sp typ rela ls\n",
    "                 gloss\n",
    "                 domain vt lex\n",
    "                 number language\n",
    "              ''')\n",
    "\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom function to id weqetal verbs and for subject isolation\n",
    "from my_functions.verbs import is_weqt \n",
    "from my_functions.phrases import is_subs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset for Time Phrases\n",
    "\n",
    "The time phrase cannot occur inside a clause with more than 1 time phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3712 time expressions ready for proccessing...\n"
     ]
    }
   ],
   "source": [
    "timexes = []\n",
    "\n",
    "for phrase in F.function.s('Time'):\n",
    "    \n",
    "    # skip non-hebrew phrases\n",
    "    language = F.language.v(L.d(phrase, otype='word')[0])\n",
    "    if language != 'hbo':\n",
    "        continue\n",
    "    \n",
    "    # skip phrases with >1 TP in its clause\n",
    "    clause = L.u(phrase, otype='clause')[0]\n",
    "    phrase_functs = [F.function.v(ph) for ph in L.d(clause, otype='phrase')]\n",
    "    if phrase_functs.count('Time') > 1:\n",
    "        continue\n",
    "        \n",
    "    # append the relevant data\n",
    "    timexes.append((clause, phrase))\n",
    "\n",
    "print(len(timexes), 'time expressions ready for proccessing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Time Phrases\n",
    "\n",
    "Retrieve the phrase/clause level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3712 rows ready for export...\n",
      "\n",
      "sample:  ['Genesis', 19, 34, 429263, 656759, '>MC', 'ø', 0, 'ø', 0, 'ø', 0, '>MC/', 1, 'advb', 'Q', 'CKB[', 'perf', 2, 3, 'I']\n"
     ]
    }
   ],
   "source": [
    "# header for csv export & and how data should be assembled\n",
    "header = ['book', 'chapter', 'verse', 'clause.n', 'phrase.n', \n",
    "          'phrase.trans', 'preps', 'num.preps', 'subs',\n",
    "          'num.subs', 'quants', 'num.quants', 'advbs', \n",
    "          'num.advbs','pdp.pattern', 'domain', \n",
    "          'verb.lex', 'verb.tense', 'position.at.vb', \n",
    "          'position.at.cl', 'cl.dependency']\n",
    "\n",
    "# put data here for csv export\n",
    "rows = []\n",
    "\n",
    "# get row data per time phrase\n",
    "for clause_n, phrase_n in timexes:\n",
    "\n",
    "    book, chapter, verse = T.sectionFromNode(clause_n)\n",
    "        \n",
    "        \n",
    "    # // phrase data //\n",
    "    \n",
    "    # transcription\n",
    "    ph_words = L.d(phrase_n, otype='word')\n",
    "    ph_trans = T.text(ph_words, fmt='lex-trans-plain').strip()\n",
    "    \n",
    "    # substantives\n",
    "    subs = [F.lex.v(w) for w in ph_words if is_subs(w)] # w/ custom funct. is_subs()\n",
    "    num_subs = len(subs)\n",
    "    subs_txt = '|'.join(subs) or 'ø'\n",
    "    \n",
    "    # prepositions\n",
    "    preps = [F.lex.v(w) for w in ph_words if F.pdp.v(w) == 'prep']\n",
    "    num_preps = len(preps)\n",
    "    preps_txt = '|'.join(preps) or 'ø'\n",
    "    \n",
    "    # quantities (card == \"cardinal number\")\n",
    "    quants = [F.lex.v(w) for w in ph_words if F.ls.v(w) == 'card']\n",
    "    num_quants = len(quants)\n",
    "    quants_txt = '|'.join(quants) or 'ø'\n",
    "    \n",
    "    # adverbs\n",
    "    advbs = [F.lex.v(w) for w in ph_words if F.pdp.v(w) == 'advb']\n",
    "    num_advbs = len(advbs)\n",
    "    advbs_txt = '|'.join(advbs) or 'ø'\n",
    "    \n",
    "    # part of speech patterns\n",
    "    pdp_pattern = '-'.join(F.pdp.v(w) for w in L.d(phrase_n, otype='word'))\n",
    "    \n",
    "    # // clause data // \n",
    "    \n",
    "    domain = F.domain.v(clause_n)\n",
    "        \n",
    "    # get verb lex and its tense\n",
    "    pred_functs = {'Pred', 'PreO', 'PreS', 'PtcO'}\n",
    "    verb = [word for phrase in L.d(clause_n, otype='phrase')\n",
    "               for word in L.d(phrase, otype='word')\n",
    "               if F.pdp.v(word) == 'verb' \n",
    "               and F.function.v(phrase) in pred_functs\n",
    "           ]\n",
    "    if verb:\n",
    "        verb_n = verb[0]\n",
    "        verb_lex = F.lex.v(verb_n)\n",
    "        verb_tense = F.vt.v(verb_n) if not is_weqt(verb_n) else 'weqt' # + hacked weqetal\n",
    "    else:\n",
    "        verb_lex = 'ø'\n",
    "        verb_tense = 'ø'\n",
    "        \n",
    "    # time-phrase position to verb\n",
    "    pos_at_vb = 1 if phrase_n < verb_n else 2\n",
    "    pos_at_cl = F.number.v(phrase_n)\n",
    "    \n",
    "    # clause dependency\n",
    "    cl_dependency = 'I' if F.rela.v(clause_n) == 'NA' else 'D'\n",
    "    \n",
    "    \n",
    "    # // ship it //\n",
    "    \n",
    "    # package the row and append it\n",
    "    row = [book, chapter, verse, clause_n, phrase_n,\n",
    "           ph_trans, preps_txt, num_preps, subs_txt,\n",
    "           num_subs, quants_txt, num_quants, advbs_txt, num_advbs,\n",
    "           pdp_pattern, domain, verb_lex, verb_tense, \n",
    "           pos_at_vb, pos_at_cl, cl_dependency]\n",
    "           \n",
    "    rows.append(row)\n",
    "    \n",
    "print(len(rows), 'rows ready for export...')\n",
    "print()\n",
    "print('sample: ', rows[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Time Phrases\n",
    "\n",
    "### A. Export Spreadsheets for Manual Annotations\n",
    "\n",
    "Use the lexemes for prepositions/substantives|adverbs in the dataset.\n",
    "\n",
    "Put common lexemes at the top of the spreadsheets. Thus, make a count first and sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep annotations file present...doing nothing.\n",
      "subs annotations file present...doing nothing.\n"
     ]
    }
   ],
   "source": [
    "prep_lexemes = collections.Counter()\n",
    "subs_lexemes = collections.Counter()\n",
    "\n",
    "# dicts to inspect the texts these items are coming from\n",
    "prep_samples = collections.defaultdict(list)\n",
    "subs_samples = collections.defaultdict(list)\n",
    "\n",
    "# indices for desired elements\n",
    "prep_i = header.index('preps') \n",
    "subs_i = header.index('subs')\n",
    "advb_i = header.index('advbs')\n",
    "\n",
    "# get data from the spreadsheet above\n",
    "for phrase_dat in rows:\n",
    "    \n",
    "    if phrase_dat[prep_i] != 'ø':\n",
    "        prep_lexemes[phrase_dat[prep_i]] += 1\n",
    "        prep_samples[phrase_dat[prep_i]].append('{} {}:{}'.format(phrase_dat[0], phrase_dat[1], phrase_dat[2]))\n",
    "        \n",
    "    if phrase_dat[subs_i] != 'ø':\n",
    "        subs_lexemes[phrase_dat[subs_i]] += 1\n",
    "        subs_samples[phrase_dat[subs_i]].append('{} {}:{}'.format(phrase_dat[0], phrase_dat[1], phrase_dat[2]))\n",
    "        \n",
    "    if phrase_dat[advb_i] != 'ø':\n",
    "        subs_lexemes[phrase_dat[advb_i]] += 1\n",
    "        subs_samples[phrase_dat[advb_i]].append('{} {}:{}'.format(phrase_dat[0], phrase_dat[1], phrase_dat[2]))\n",
    "\n",
    "# export 4 spreadsheets\n",
    "# 2 for semantic annotations\n",
    "# 2 for example passages for each\n",
    "\n",
    "# prepositions\n",
    "prep_file = 'manual_annotations/prep_annotations'\n",
    "\n",
    "if not os.path.exists(prep_file+'.tsv'): # prevent overwrite\n",
    "    \n",
    "    print('writing', prep_file, '...')\n",
    "    \n",
    "    with open(prep_file+'.tsv', 'w') as outfile:\n",
    "\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "\n",
    "        sort_preps = sorted(prep_lexemes.items(), key=lambda k: k[-1], reverse=True)\n",
    "\n",
    "        writer.writerow(['lexeme', 'annotation'])\n",
    "        writer.writerows([prep[0], ''] for prep in sort_preps)\n",
    "\n",
    "    # example passages file\n",
    "    with open(prep_file+'_examples.txt', 'w') as outfile:\n",
    "                        \n",
    "        for prep, count in sort_preps:\n",
    "            \n",
    "            outfile.write(prep+'\\n')\n",
    "            outfile.write('; '.join(prep_samples[prep]))\n",
    "            outfile.write('\\n')\n",
    "            outfile.write('-'*100)\n",
    "            outfile.write('\\n'*2)\n",
    "            \n",
    "else:\n",
    "    print('prep annotations file present...doing nothing.')\n",
    "            \n",
    "# substantives\n",
    "\n",
    "subs_file = 'manual_annotations/subs_annotations'\n",
    "\n",
    "if not os.path.exists(subs_file+'.tsv'): # prevent overwrite\n",
    "\n",
    "    print('writing', subs_file, '...')\n",
    "    \n",
    "    with open(subs_file+'.tsv', 'w') as outfile:\n",
    "\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "\n",
    "        sort_subs = sorted(subs_lexemes.items(), key=lambda k: k[-1], reverse=True)\n",
    "\n",
    "        writer.writerow(['lexeme', 'annotation'])\n",
    "        writer.writerows([subs[0], ''] for subs in sort_subs)\n",
    "    \n",
    "    # example passages file\n",
    "    with open(subs_file+'_examples.txt', 'w') as outfile:\n",
    "                        \n",
    "        for subs, count in sort_subs:\n",
    "            \n",
    "            outfile.write(subs+'\\n')\n",
    "            outfile.write('; '.join(subs_samples[subs]))\n",
    "            outfile.write('\\n')\n",
    "            outfile.write('-'*100)\n",
    "            outfile.write('\\n'*2)\n",
    "\n",
    "else:\n",
    "    print('subs annotations file present...doing nothing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The manual annotations are complete.** See the [manual annotations readme](manual_annotations/readme.md) for a report on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Identify Time Phrase Patterns \n",
    "\n",
    "Manual annotations are extracted and applied to the row. Instead of looking for ordered elements, a set of tags will be generated simply by the presence of an annotated element in a row. The tags are unordered and there are no repeated elements.\n",
    "\n",
    "#### Import Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 prep. annotations ready...\n",
      "88 subs. annotations ready...\n"
     ]
    }
   ],
   "source": [
    "# import manual preposition annotations\n",
    "with open(prep_file + '.tsv', 'r') as infile:\n",
    "    \n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    \n",
    "    next(reader) # skip header\n",
    "    \n",
    "    prep_classes = dict(row for row in reader if row[1]) # take only annotated rows, ignore blank\n",
    "    \n",
    "    print(len(prep_classes), 'prep. annotations ready...')\n",
    "    \n",
    "# import manual substantive annotations\n",
    "with open(subs_file + '.tsv', 'r') as infile:\n",
    "    \n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    \n",
    "    next(reader) # skip header\n",
    "    \n",
    "    subs_classes = dict(row for row in reader if row[1]) # ignore blank\n",
    "    \n",
    "    print(len(subs_classes), 'subs. annotations ready...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'internal'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_classes['B'] # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify Data Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special case is made for the classes `demonstrative_near` and `demonstrative_far`. Some demonstratives were not included in the adverbs and are also added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('>L===', 'these'),\n",
      " ('>LH', 'these'),\n",
      " ('HJ>', 'she'),\n",
      " ('HM', 'they'),\n",
      " ('HMH', 'they'),\n",
      " ('HNH=', 'they'),\n",
      " ('HW>', 'he'),\n",
      " ('LZ', 'this there'),\n",
      " ('LZH', 'this there'),\n",
      " ('LZW', 'this there'),\n",
      " ('Z>T', 'this'),\n",
      " ('ZH', 'this'),\n",
      " ('ZH=', 'this'),\n",
      " ('ZW=', 'this')}\n"
     ]
    }
   ],
   "source": [
    "# print all demonstrative pronouns for sorting into near and far pronouns\n",
    "\n",
    "demon_pronouns = set((F.lex.v(w), F.gloss.v(L.u(w, otype='lex')[0])) for w in F.pdp.s('prde')\n",
    "                        if F.language.v(w) == 'hbo'\n",
    "                    )\n",
    "\n",
    "pprint(demon_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "demon_near = {'>L===', '>LH', 'LZ', \n",
    "             'LZH', 'LZW', 'Z>T', \n",
    "              'ZH', 'ZH=', 'ZW='}\n",
    "\n",
    "demon_far = {'HJ>', 'HM', 'HMH', \n",
    "             'HNH=', 'HW>'}\n",
    "\n",
    "demon_classes = {}\n",
    "\n",
    "for demon in demon_near | demon_far:\n",
    "    \n",
    "    if demon in demon_near:\n",
    "        demon_classes[demon] = 'demonstrative_near'\n",
    "    else:\n",
    "        demon_classes[demon] = 'demonstrative_far'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for annotation matches and add to the dataset in the form of a new column. Add a null value for timexes without a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3248 rows tagged...\n"
     ]
    }
   ],
   "source": [
    "header += ['timex.group'] # add new data column to dataset header\n",
    "\n",
    "tagged = 0 # keep count of matches\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    # grab data for comparison\n",
    "    prep = row[prep_i]\n",
    "    subs = row[subs_i]\n",
    "    advb = row[advb_i]\n",
    "    \n",
    "    # extract any demonstratives from the phrase and classify them\n",
    "    phrase_i = header.index('phrase.n')\n",
    "    demons = [F.lex(w) for w in L.d(phrase_i, otype='word')\n",
    "                 if F.pdp.v(w) == 'prde'\n",
    "             ]\n",
    "    if len(demons) > 1 or not demons: # skip plural demonstratives, the same is done for other constructions\n",
    "        demon = ''\n",
    "    else:\n",
    "        demon = demon_classes[demons[0]]\n",
    "    \n",
    "    # classify prepositions, substantives, and adverbs\n",
    "    prep_tag = prep_classes.get(prep, '')\n",
    "    subs_tag = subs_classes.get(subs, '')\n",
    "    advb_tag = subs_classes.get(advb, '')\n",
    "    \n",
    "    tag_elements = [prep_tag] + sorted({subs_tag, advb_tag, demon})\n",
    "    tag_elements = [el for el in tag_elements if el] # clean out empty tags\n",
    "\n",
    "    # create the tag\n",
    "    group_tag = '.'.join(tag_elements) or 'ø'\n",
    "    \n",
    "    if group_tag != 'ø':\n",
    "        tagged += 1\n",
    "    \n",
    "    # add the tag to the dataset\n",
    "    row.append(group_tag)\n",
    "    \n",
    "print(tagged, 'rows tagged...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Timex Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Export the .csv\n",
    "with open('time_phrases.csv', 'w') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset for Verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82980 rows ready for export...\n",
      "\n",
      "sample:  ['Genesis', 1, 5, 427564, 69, 'QR>[', 'perf', 'N', 2, 'I']\n"
     ]
    }
   ],
   "source": [
    "header = ['book', 'chapter', 'verse', 'clause.n', \n",
    "          'verb.n', 'lexeme', 'tense', 'domain', \n",
    "          'position.at.cl', 'cl.dependency']\n",
    "\n",
    "rows = []\n",
    "\n",
    "# gather verb data by clause\n",
    "for clause in F.otype.s('clause'):\n",
    "\n",
    "    book, chapter, verse = T.sectionFromNode(clause)\n",
    "    \n",
    "    # skip clauses with a time phrase\n",
    "    clause_functs = set(F.function.v(phrase) for phrase in L.d(clause, otype='phrase'))\n",
    "    if 'Time' in clause_functs:\n",
    "        continue\n",
    "    \n",
    "    # skip non-Hebrew clauses\n",
    "    if F.language.v(L.d(clause, otype='word')[0]) != 'hbo':\n",
    "        continue\n",
    "    \n",
    "    # extract verb data\n",
    "    pred_functs = {'Pred', 'PreO', 'PreS', 'PtcO'}\n",
    "    # isolate verb from predicate phrase:\n",
    "    verb = [word for phrase in L.d(clause, otype='phrase')\n",
    "               for word in L.d(phrase, otype='word')\n",
    "               if F.pdp.v(word) == 'verb' \n",
    "               and F.function.v(phrase) in pred_functs\n",
    "           ]\n",
    "    \n",
    "    if verb: # get tense/lexeme\n",
    "        verb_n = verb[0]\n",
    "        verb_lex = F.lex.v(verb_n)\n",
    "        verb_tense = F.vt.v(verb_n) if not is_weqt(verb_n) else 'weqt' # + hacked weqetal\n",
    "\n",
    "    else: # handle verbless clauses\n",
    "        verb_n = 0\n",
    "        verb_lex = 'ø'\n",
    "        verb_tense = 'ø'\n",
    "    \n",
    "    # clause level data\n",
    "    domain = F.domain.v(clause)\n",
    "    pos_at_cl = F.number.v(phrase)\n",
    "    cl_dependency = 'I' if F.rela.v(clause) == 'NA' else 'D'\n",
    "\n",
    "    # package and save\n",
    "    row = [book, chapter, verse, clause, verb_n, \n",
    "           verb_lex, verb_tense, domain, pos_at_cl, \n",
    "           cl_dependency]\n",
    "    \n",
    "    rows.append(row)\n",
    "    \n",
    "print(len(rows), 'rows ready for export...')\n",
    "print()\n",
    "print('sample: ', rows[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Tense Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "with open('verbs.csv', 'w') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset for All Clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86808 rows ready for export...\n",
      "\n",
      "sample:  ['Genesis', 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "header = ['book', 'chapter', 'verse', 'has.timex']\n",
    "rows = []\n",
    "\n",
    "for clause in F.otype.s('clause'):\n",
    "    \n",
    "    # skip non-hebrew\n",
    "    language = F.language.v(L.d(clause, otype='word')[0])\n",
    "    if language != 'hbo':\n",
    "        continue\n",
    "    \n",
    "    book, chapter, verse = T.sectionFromNode(clause)\n",
    "    \n",
    "    has_timex = 1 if 'Time' in set(F.function.v(ph) for ph in L.d(clause, otype='phrase')) else 2\n",
    "    \n",
    "    row = [book, chapter, verse, has_timex]\n",
    "    \n",
    "    rows.append(row)\n",
    "    \n",
    "print(len(rows),'rows ready for export...')\n",
    "print()\n",
    "print('sample: ', rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "with open('all_clauses.csv', 'w') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)\n",
    "    \n",
    "print('done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
