{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export BHSA Data for Processing\n",
    "\n",
    "Use Text-Fabric to export a time-expression (timex) dataset and a verb dataset.\n",
    "\n",
    "In all cases, the data is only selected from Biblical Hebrew.\n",
    "\n",
    "## 1. Dataset for Time Expressions\n",
    "* **Select phrases with the function of `Time`.**\n",
    "    * Exclude phrases that occur in clauses with multiple time-function phrases.\n",
    "    <br><br>\n",
    "* **Features of the phrase**\n",
    "    * lexeme/count of prepositions, substantives, and quantifiers (cardinal numbers).\n",
    "    * part of speech patterns within the phrase\n",
    "    <br><br>\n",
    "* **Features of the enclosing clause**\n",
    "    * domain\n",
    "    * lex and tense of verb if present\n",
    "    * position of phrase in relation to the verb \n",
    "    * position of phrase in relation to the clause\n",
    "    * main vs. subordinate\n",
    "    \n",
    "## 2. Dataset for Verbs\n",
    "\n",
    "In order to compare the time phrase dataset with the broader tendencies of verbs in the Hebrew Bible, another export contains:\n",
    "* **Features of verbs in the HB**\n",
    "    * verb must be in a predicate phrase\n",
    "    * tense and lexeme\n",
    "    <br><br>\n",
    "* **Features of the enclosing clause**\n",
    "    * domain\n",
    "    * position of verb's phrase in relation to the clause\n",
    "    * main vs. subordinate\n",
    "    \n",
    "## 3. Dataset for All Clauses\n",
    "* **A simple export where each row represents a clause, has a T/F value for the presence of a time phrase**.\n",
    "    * Include book, chapter, and verse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import sys, path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put custom functions in path\n",
    "if __name__ == '__main__' and __package__ is None:\n",
    "    sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.1.1\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "114 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='~/github/etcbc/bhsa/tf', modules='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.01s B book                 from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B chapter              from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B verse                from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.08s B function             from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.03s B kind                 from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.19s B pdp                  from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.17s B sp                   from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.24s B typ                  from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.23s B rela                 from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.14s B ls                   from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.02s B domain               from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.13s B vt                   from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.15s B lex                  from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.29s B number               from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.13s B language             from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 108 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  7.99s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "                 book chapter verse\n",
    "                 function kind\n",
    "                 pdp sp typ rela ls\n",
    "                 domain vt lex\n",
    "                 number language\n",
    "              ''')\n",
    "\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom function to id weqetal verbs and for subject isolation\n",
    "from my_functions.verbs import is_weqt \n",
    "from my_functions.phrases import is_subs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset for Time Phrases\n",
    "\n",
    "The time phrase cannot occur inside a clause with more than 1 time phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3712 time expressions ready for proccessing...\n"
     ]
    }
   ],
   "source": [
    "timexes = []\n",
    "\n",
    "for phrase in F.function.s('Time'):\n",
    "    \n",
    "    # skip non-hebrew phrases\n",
    "    language = F.language.v(L.d(phrase, otype='word')[0])\n",
    "    if language != 'hbo':\n",
    "        continue\n",
    "    \n",
    "    # skip phrases with >1 TP in its clause\n",
    "    clause = L.u(phrase, otype='clause')[0]\n",
    "    phrase_functs = [F.function.v(ph) for ph in L.d(clause, otype='phrase')]\n",
    "    if phrase_functs.count('Time') > 1:\n",
    "        continue\n",
    "        \n",
    "    # append the relevant data\n",
    "    timexes.append((clause, phrase))\n",
    "\n",
    "print(len(timexes), 'time expressions ready for proccessing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Time Phrases\n",
    "\n",
    "Retrieve the phrase/clause level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3712 rows ready for export...\n",
      "\n",
      "sample:  ['Genesis', 19, 34, 429263, 656759, '>MC', 'ø', 0, 'ø', 0, 'ø', 0, 'ø', 0, 'advb', 'subs', 'Q', 'CKB[', 'perf', 2, 3, 'I']\n"
     ]
    }
   ],
   "source": [
    "# header for csv export & and how data should be assembled\n",
    "header = ['book', 'chapter', 'verse', 'clause.n', 'phrase.n', \n",
    "          'phrase.trans', 'preps', 'num.preps', 'subs',\n",
    "          'num.subs', 'quants', 'num.quants', 'advbs', \n",
    "          'num.advbs','pdp.pattern', 'sp.pattern', 'domain', \n",
    "          'verb.lex', 'verb.tense', 'position.at.vb', \n",
    "          'position.at.cl', 'cl.dependency']\n",
    "\n",
    "# put data here for csv export\n",
    "rows = []\n",
    "\n",
    "# get row data per time phrase\n",
    "for clause_n, phrase_n in timexes:\n",
    "\n",
    "    book, chapter, verse = T.sectionFromNode(clause_n)\n",
    "        \n",
    "        \n",
    "    # // phrase data //\n",
    "    \n",
    "    # transcription\n",
    "    ph_words = L.d(phrase_n, otype='word')\n",
    "    ph_trans = T.text(ph_words, fmt='lex-trans-plain').strip()\n",
    "    \n",
    "    # substantives\n",
    "    subs = [F.lex.v(w) for w in ph_words if is_subs(w)] # w/ custom funct. is_subs()\n",
    "    num_subs = len(subs)\n",
    "    subs_txt = '|'.join(subs) or 'ø'\n",
    "    \n",
    "    # prepositions\n",
    "    preps = [F.lex.v(w) for w in ph_words if F.sp.v(w) == 'prep']\n",
    "    num_preps = len(preps)\n",
    "    preps_txt = '|'.join(preps) or 'ø'\n",
    "    \n",
    "    # quantities (card == \"cardinal number\")\n",
    "    quants = [F.lex.v(w) for w in ph_words if F.ls.v(w) == 'card']\n",
    "    num_quants = len(quants)\n",
    "    quants_txt = '|'.join(quants) or 'ø'\n",
    "    \n",
    "    # adverbs\n",
    "    advbs = [F.lex.v(w) for w in ph_words if F.sp.v(w) == 'advb']\n",
    "    num_advbs = len(advbs)\n",
    "    advbs_txt = '|'.join(advbs) or 'ø'\n",
    "    \n",
    "    # part of speech patterns\n",
    "    pdp_pattern = '-'.join(F.pdp.v(w) for w in L.d(phrase_n, otype='word'))\n",
    "    sp_pattern = '-'.join(F.sp.v(w) for w in L.d(phrase_n, otype='word'))\n",
    "    \n",
    "    # // clause data // \n",
    "    \n",
    "    domain = F.domain.v(clause_n)\n",
    "        \n",
    "    # get verb lex and its tense\n",
    "    pred_functs = {'Pred', 'PreO', 'PreS', 'PtcO'}\n",
    "    verb = [word for phrase in L.d(clause_n, otype='phrase')\n",
    "               for word in L.d(phrase, otype='word')\n",
    "               if F.pdp.v(word) == 'verb' \n",
    "               and F.function.v(phrase) in pred_functs\n",
    "           ]\n",
    "    if verb:\n",
    "        verb_n = verb[0]\n",
    "        verb_lex = F.lex.v(verb_n)\n",
    "        verb_tense = F.vt.v(verb_n) if not is_weqt(verb_n) else 'weqt' # + hacked weqetal\n",
    "    else:\n",
    "        verb_lex = 'ø'\n",
    "        verb_tense = 'ø'\n",
    "        \n",
    "    # time-phrase position to verb\n",
    "    pos_at_vb = 1 if phrase_n < verb_n else 2\n",
    "    pos_at_cl = F.number.v(phrase_n)\n",
    "    \n",
    "    # clause dependency\n",
    "    cl_dependency = 'I' if F.rela.v(clause_n) == 'NA' else 'D'\n",
    "    \n",
    "    \n",
    "    # // ship it //\n",
    "    \n",
    "    # package the row and append it\n",
    "    row = [book, chapter, verse, clause_n, phrase_n,\n",
    "           ph_trans, preps_txt, num_preps, subs_txt,\n",
    "           num_subs, quants_txt, num_quants, advbs_txt, num_advbs,\n",
    "           pdp_pattern, sp_pattern, domain, verb_lex, \n",
    "           verb_tense, pos_at_vb, pos_at_cl, cl_dependency]\n",
    "           \n",
    "    rows.append(row)\n",
    "    \n",
    "print(len(rows), 'rows ready for export...')\n",
    "print()\n",
    "print('sample: ', rows[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Timex Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export the .csv\n",
    "with open('time_phrases.csv', 'w') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset for Verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59726 rows ready for export...\n",
      "\n",
      "sample:  ['Genesis', 2, 16, 427735, 977, '>KL[', 'perf', 'Q', 3, 'I']\n"
     ]
    }
   ],
   "source": [
    "header = ['book', 'chapter', 'verse', 'clause.n', \n",
    "          'verb.n', 'lexeme', 'tense', 'domain', \n",
    "          'position.at.cl', 'cl.dependency']\n",
    "\n",
    "rows = []\n",
    "\n",
    "# gather verb data\n",
    "for verb in F.pdp.s('verb'):\n",
    "\n",
    "    book, chapter, verse = T.sectionFromNode(verb)\n",
    "    clause_n = L.u(verb, otype='clause')[0] # clause node for filtering (below)\n",
    "\n",
    "    # skip non-Hebrew verbs\n",
    "    if F.language.v(verb) != 'hbo':\n",
    "        continue\n",
    "\n",
    "    # skip verbs in a clause with a time phrase\n",
    "    clause_functs = set(F.function.v(phrase) for phrase in L.d(clause_n, otype='phrase'))\n",
    "    if 'Time' in clause_functs:\n",
    "        continue\n",
    "    \n",
    "    # skip verbs without a predicate phrase function\n",
    "    phrase = L.u(verb, otype='phrase')[0]\n",
    "    pred_functs = {'Pred', 'PreO', 'PreS', 'PtcO'}\n",
    "    if F.function.v(phrase) not in pred_functs:\n",
    "        continue\n",
    "        \n",
    "    # verb level data\n",
    "    tense = F.vt.v(verb_n) if not is_weqt(verb_n) else 'weqt' # + hacked weqetal\n",
    "    lex = F.lex.v(verb)\n",
    "    \n",
    "    # clause level data\n",
    "    domain = F.domain.v(clause_n)\n",
    "    pos_at_cl = F.number.v(phrase)\n",
    "    cl_dependency = 'I' if F.rela.v(clause_n) == 'NA' else 'D'\n",
    "\n",
    "    # package and save\n",
    "    row = [book, chapter, verse, clause_n, verb, \n",
    "           lex, tense, domain, pos_at_cl, \n",
    "           cl_dependency]\n",
    "    \n",
    "    rows.append(row)\n",
    "    \n",
    "print(len(rows), 'rows ready for export...')\n",
    "print()\n",
    "print('sample: ', rows[124])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Tense Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('verbs.csv', 'w') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset for All Clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86808 rows ready for export...\n",
      "\n",
      "sample:  ['Genesis', 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "header = ['book', 'chapter', 'verse', 'has.timex']\n",
    "rows = []\n",
    "\n",
    "for clause in F.otype.s('clause'):\n",
    "    \n",
    "    # skip non-hebrew\n",
    "    language = F.language.v(L.d(clause, otype='word')[0])\n",
    "    if language != 'hbo':\n",
    "        continue\n",
    "    \n",
    "    book, chapter, verse = T.sectionFromNode(clause)\n",
    "    \n",
    "    has_timex = 1 if 'Time' in set(F.function.v(ph) for ph in L.d(clause, otype='phrase')) else 2\n",
    "    \n",
    "    row = [book, chapter, verse, has_timex]\n",
    "    \n",
    "    rows.append(row)\n",
    "    \n",
    "print(len(rows),'rows ready for export...')\n",
    "print()\n",
    "print('sample: ', rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_clauses.csv', 'w') as outfile:\n",
    "    \n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
